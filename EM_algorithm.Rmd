---
title: "EM algorithm for multivariate data"
author: "Mariana Chaves and Franz Franco Gallo"
output:
  html_document:
    code_download: yes
    highlight: tango
    number_sections: no
    theme: flatly
    toc: TRUE
    toc_float: TRUE
editor_options: 
  chunk_output_type: inline
---

# Introduction

In this work, we implement the EM-algorithm for GMM. It is applied to multivariate data, more specifically the wine dataset available in the pgmm package. This dataset contains chemical characteristics about three types of wine. We use AIC, BIC and the likelihood as metrics to identify an adequated number of clusters. To assess the quality of the clustering, we use the function classError and adjustedRandIndex from the Mclust package. We compare the results of GMM with another clustering algorithm (k-means), and against the ground truth, that is the original wine type.   

```{r libraries, include = FALSE}
library(pgmm)
library(ellipse)
library(ggplot2)
library(ggpubr)
library(mclust)
library(dplyr)
library(reshape)
library(mvtnorm)
library(dplyr)
```


# Useful functions

In this section we define the functions that will help us in our analysis. 

* `logsumexp` computes the log of the sum of the exponential of each entry of a vector $x$. This is useful for the computation of the log-likelihood and the responsibilities $\gamma(z_{nk})$.
* We create two functions, `initialization.random_centroids` and `initialization.k_means`, for the initialization of the parameters $\mu_k$, $\sigma_k$ and $\pi_k$. Given $K$, the number of clusters, `initialization.random_centroids` randomly samples $K$ points of the data and sets them as the initialization of $\mu_1, \ldots , \mu_k$. The variance-covariance matrix of the data $X$ serves as initialization for each $sigma_k$. Each $\pi_k$ is set to $1/K$. The second function, `initialization.k_means`, runs k-means on the data $X$. Then it uses the final centroids produced by k-means to initialize each $\mu_k$. It computes the variance-covariance matrices of the clusters found by k-means to initialize each $\sigma_k$. Finally, each $pi_k$ is initiliazed according to the number of elements in the clusters.
* Given a dataset $X$ and a number of desired clusters $K$, `EM_algorithm` computes the EM algorithm. One of the previously described initializations must be chosen.     
* `classify` takes the resulting means ($\mu_1, \ldots, \mu_K$), variances ($\sigma_1, \ldots, \sigma_K$), and proportions ($\pi_1, \ldots, \pi_K$) of the EM algorithm and classifies the points in $K$ clusters.
* `metrics` computes the likelihood, AIC and BIC. 
* `crossvalidation.EM` performs cross-validation on the EM-algorithm. It evaluates the log-likelihood.   
* Subsequently we present several functions related to plotting. 


```{r functions}
logsumexp = function(x) {
  "
  This function computes the log of the sum of the exponentials of each entry of a vector x
  
  Input:
  x: a vector x
  "
  y = max(x)
  y + log(sum(exp(x-y)))
}

#Initialization functions

initialization.random_centroids = function(X,K){
  "
  This function defines the initialization of mu, sigma and prop.
  
  To initialize the mus, K random data points for X are sampled.
  To initialize the sigmas, we use the variance-covariance matrix of X.
  The props are initialized as 1/K 
  
  Inputs:
  X: the data
  K: the number of groups
  "
  mu = X[c(sample.int(n=dim(X)[1], size=K)),]
  sigma = lapply(1:K, FUN = function(i) {var(X)})
  prop = rep(1/K,K) 
  
  return(list(mu = mu, sigma = sigma, prop = prop))
  
}


initialization.k_means = function(X,K){
  "
  This function defines the initialization of mu, sigma and prop.
  
  First k-means is applied to the data X to define K clusters. 
  The final centroids are used as initialization for mu.
  The variance-covariance matrix of each cluster are used as initialization for sigma. 
  The proportion of elements in each cluster are used as initialization for prop.
  
  Inputs:
  X: the data
  K: the number of groups
  "
  n = dim(X)[1]
  kmeans_result = kmeans(X, K, nstart = 25)
  mu = as.matrix(kmeans_result$centers)
  sigma = lapply(1:K, FUN = function(i){var(X[kmeans_result$cluster == i,])})
  prop = kmeans_result$size / n
  
  return(list(mu = mu, sigma = sigma, prop = prop))  
}


metrics = function(X,K,mu,sigma,prop){
  "
  This function computes the log-likelihood, AIC and BIC for a GMM given the data, the props, the means, and variances of the gaussians.
  
  Inputs:
  X: the data
  K: the number of groups
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  
  Outputs:
  loglik: the final log-likelihood
  aic: AIC of the model
  bic: BIC of the model  
  "
  n = dim(X)[1] #number of observations
  d = dim(X)[2] #number of features
  eta = (K-1) + (K*d) + (K*d*(d-1)/2) #penalty term 
  
  # Compute Likelihood
  log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
  loglik = sum(apply(log_gamma_numerator, 1, logsumexp))
  
  # Compute AIC and BIC
  aic = loglik - eta
  bic = loglik - 0.5 * eta * log(n)
  
  return(list(loglik = loglik,
              aic = aic,
              bic = bic))
} 

EM_algorithm = function(X,K,max_it=1000, initialization, number_of_initialization = 10){
  
  "
  This function computes the EM-algorithm
  
  Note that the EM-algorith is sensible to the initialization of the parameters. 
  That is, different initializations can let to different local maxima of the log-likelihood.
  'number of initialization' defines the number of times to run the EM-algorithm with different initializations.
  The function outputs the results related to the best maxima found. 
  
  Inputs:
  X: the data
  K: the number of groups
  max_it: maximum of iterations (defaults to 1000)
  initialization: 'random centroids' or 'k_means'
  number_of_initialization: 
  
  Outputs:
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  loglik: the final log-likelihood
  loglik_hist: a vector with the log-likelihood in each iteration
  aic: AIC of the model
  bic: BIC of the model
  "
  
  X = as.matrix(X)
  n = dim(X)[1] #number of observations
  d = dim(X)[2] #number of features
  
  best_maximum = -Inf
  for (j in 1:number_of_initialization){
    
    gamma = matrix(NA,n,K)
    prev_loglik = -Inf
    loglik_hist = c() # vector to save history of log-likelihood
    mu_hist <- matrix(ncol=K*d, nrow=max_it)
      
    # Initialization of parameters
    if (initialization == "random_centroids"){
      init.param = initialization.random_centroids(X,K)
    } else if (initialization == "k_means") {
      init.param = initialization.k_means(X,K)
    }
    mu = as.matrix(init.param$mu)
    sigma = init.param$sigma
    prop = init.param$prop
    
    
    for (i in 1:max_it){
      
      # S step
      log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
      log_gamma = log_gamma_numerator - apply(log_gamma_numerator, 1, logsumexp)
      gamma = exp(log_gamma)
    
      # M step
       for (k in 1:K){
        nk = sum(gamma[,k])
        prop[k] = nk/n
        mu[k,] = colSums(gamma[,k]*X)/nk
        sigma[[k]] = t(sweep(X,2,mu[k,]))%*%diag(gamma[,k])%*%(sweep(X,2,mu[k,])) / nk 
       }
      
      
      # Compute Likelihood, AIC and BIC
      results_metrics = metrics(X,K,mu,sigma,prop)
      loglik = results_metrics$loglik
      aic = results_metrics$aic
      bic = results_metrics$bic
      
      # Save history of likelihood and mu
      loglik_hist = append(loglik_hist,loglik)
      mu_hist[i,] <- as.vector(mu)
    
      # Stop process if likelihood didn't change
      if (prev_loglik == loglik){break}
      
      prev_loglik = loglik
    
    }
    
    
    mu_hist = mu_hist[rowSums(is.na(mu_hist)) == 0, ]
    mu_hist = as.data.frame(mu_hist)
      
    
    results = list(mu = mu,
                   mu_hist = mu_hist,
                sigma = sigma,
                prop = prop,
                loglik = loglik,
                loglik_hist = loglik_hist,
                aic = aic,
                bic = bic
                )
  
  # If this initialization found a better maxima, save the results and new best maximum. 
  if (best_maximum < results$loglik)
    {
    best_maximum = results$loglik
    best_results = results
    }
  
  }
  
  return(best_results)
  
  
}


classify = function(X,K,mu,sigma,prop){
   "
  This funtion classifies the data points according to the results of the EM-algorithm
  
  Inputs:
  X: the data
  K: the number of groups
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  
  Outputs:
  a vector indicating the cluster to which the point belongs
  "
  
  log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
  log_gamma = log_gamma_numerator - apply(log_gamma_numerator, 1, logsumexp)
  gamma = exp(log_gamma)
  cluster = sapply(1:n, FUN = function(i){which.max(gamma[i,])})
  
  return(cluster)
}

crossvalidation.EM = function(X, K, k_fold, max_it=1000, initialization, seed = 32){
  "
  This function perform k-fold cross validation on the EM-algorithm. 
  
  Inputs:
  X: the data
  K: the number of clusters in the EM algorithm
  k_fold: the number of groups for cross validation
  max_it: maximum of iterations in the EM algorithm (defaults to 1000)
  initialization: 'random centroids' or 'k_means'
  seed: seed for the random process that assigns the datapoints to each fold 
  
  Outputs:
  logliks: vector with the loglikelihoods obtained in the validation sets
  "
  
  # Assigning each datapoint to one of k groups
  X = as.matrix(X)
  n = dim(X)[1]
  vector_groups = ceiling((1:n)/(n/k_fold))
  set.seed(seed)
  vector_groups = sample(vector_groups,n)
  
  # Variables to save results
  logliks = c()
  
  for (i in 1:k_fold){
    # training and validation split
    train = X[vector_groups!=i,]
    validation = X[vector_groups==i,]
    
    #run EM algorithm on train set
    EM_results = EM_algorithm(X = train, K=K, max_it = max_it, initialization = initialization)
    
    #get likelihood on validation set
    metric_results = metrics(X = validation, K = K, mu = EM_results$mu, sigma = EM_results$sigma, prop = EM_results$prop)
    logliks = append(logliks,metric_results$loglik)
  }
  
  return(logliks)
  
}


# Plotting functions

plot_ellipses = function(X,K,mu,sigma,groups_pred){
   "
  This funtion plot the ellipses of the GMM clustering defined by mu and sigma
  
  Inputs:
  X: data
  K: the number of clusters in the EM algorithm
  mu: mean computed by the EM algorithm
  sigma: cov matrix computed by the EM algorithm
  groups_pred: vector predicted by the EM with the cluster group for each data point
  
  Outputs:
  p: plot of the data with the GMM clusters(ellipses)
  "
  df <- data.frame(X, groups_pred)
  colnames(df) <- c("x1","x2", "group")
  for(g in c(1:K)){
    df$group[df$group==g] <- LETTERS[g]
  }
  
  #calculating ellipses
  
  clusters <- sort(unique(df$group))
  df_ell <- data.frame()
  for(g in c(1:K)){
  M=sigma[[g]]
  centre=mu[g,]
  
  df_ell <- rbind(df_ell, cbind(as.data.frame(ellipse(M, centre=centre)),group=clusters[g]))
  
  }
  colnames(df_ell) <- c("x1", "x2", "group")
  #drawing
  mu_df = data.frame(mu)
  p <- ggplot(data=df, aes(x=x1, y=x2,colour=group)) + 
    geom_point(size=1.5, alpha=.6) + 
    geom_path(data=df_ell, aes(x=x1, y=x2,colour=group), size=1, linetype=2) +
    geom_point(data=mu_df,aes(x=mu_df[,1],y=mu_df[,2]), shape=4, color='black',size=3) + 
    xlab(colnames(X)[1]) +
    ylab(colnames(X)[2]) +
    theme_bw()
    
  return(p)
}

plot_ellipses_CI = function(X,K,groups){
   "
    This funtion plot the ellipses defined by the data itself with a 95% CI
  
  Inputs:
  X: data
  K: the number of clusters
  groups: vector with the cluster group for each data point
  
  Outputs:
  p: plot of the data with its cluster
  "
  df <- data.frame(X, groups)
  colnames(df) <- c("x", "y", "group")
  for(g in c(1:K)){
    df$group[df$group==g] <- LETTERS[g]
    
  }
  
  #calculating ellipses
  clusters <- sort(unique(df$group))
  df_ell <- data.frame()
  mu_df <- matrix(ncol=2, nrow=K)
  for(g in c(1:K)){
  df_ell <- rbind(df_ell, cbind(as.data.frame(with(df[df$group==clusters[g],], ellipse(cor(x, y), 
                                           scale=c(sd(x),sd(y)), 
                                           centre=c(mean(x),mean(y))))),group=clusters[g]))
  df1<-df[df$group==clusters[g],]
  mu_df[g,] = c(mean(df1$x),mean(df1$y))
  mu_df <- data.frame(mu_df)
  }
  
  #drawing
  p <- ggplot(data=df, aes(x=x, y=y,colour=group)) + geom_point(size=1.5, alpha=.6) + geom_path(data=df_ell, aes(x=x, y=y,colour=group), size=1, linetype=2) + theme_bw() + geom_point(data=mu_df,aes(x=mu_df[,1],y=mu_df[,2]), shape=4, color='black',size=3)
    
  return(p)
}

plot_hist_mu = function(data_mu, K){
  
  data_mu = na.omit(data_mu)
  data_mu1 <- data_mu %>% select(c(1:K))
  data_mu2 <- data_mu %>% select(c((K+1):ncol(data_mu)))
  
  data_mu1$index=c(1:nrow(data_mu1))
  data_mu2$index=c(1:nrow(data_mu2))
  
  df1 <- melt(data_mu1 ,  id.vars = 'index', variable.name = 'mu_clusters')
  df2 <- melt(data_mu2 ,  id.vars = 'index', variable.name = 'mu_clusters')
  
  p1 <- ggplot(df1, aes(index, value)) + geom_point(aes(colour = variable), shape=4) + geom_path(aes(colour = variable))+ theme_bw()
  p2 <- ggplot(df2, aes(index, value)) + geom_point(aes(colour = variable), shape=4) + geom_path(aes(colour = variable))+ theme_bw()
  
  plots_mu = list(p1 = p1, p2 = p2)
  return(plots_mu)
    
}

plot_hist_loglik = function(loglik_hist){

  data_loglik = as.data.frame(loglik_hist)
  data_loglik$index=c(1:nrow(data_loglik))
  colnames(data_loglik) = c("loglik_hist", "index")
  
  p1 <- ggplot(data_loglik, aes(index,loglik_hist)) + geom_point(shape=4) + geom_path()+ theme_bw()
  
  return(p1)

}

plot_AIC_BIC_by_cluster = function(aic_vector,bic_vector,min_clusters,max_clusters){
  '
  This function plots the AIC and BIC by number of clusters. 
  
  Inputs:
  aic_vector: a vector of AICs.
  bic_vector: a vector of BICs
  min_clusters: the minimum number of clusters
  max_clusters: maximum number of clusters
  
  Output:
  p: a plot of AIC and BIC by number of clusters'
  
  
  # Define appropriate dataset
  df = data.frame(c(aic_vector,bic_vector), rep(min_clusters:max_clusters, 2), rep(c("AIC","BIC"),each = length(min_clusters:max_clusters)))
  names(df) = c('Information criteria', 'Number of clusters','Type')
  
  # plot
  p = df %>% 
    ggplot(aes(x=`Number of clusters`, y=`Information criteria`, color=Type)) + 
    geom_point(size = 3) + 
    theme_bw()
  
  return(p)
}

plot_loglik_by_cluster = function(validation_loglik,num_clusters){
  '
  This function plots the boxplots of the loglikelihoods obtained from the cross-validation process by the number of clusters. 
  The mean of the loglikelihoods is presented in red. 
  
  Input:
  validation_loglik: vector of log-likelihoods obtain from crossvalidation
  num_clusters: vector with the corresponding number of clusters associated to each log-likelihood
  
  Output: 
  p: a boxplot
  
  '
  
  # Define appropriate datasets from the plot
  df =  data.frame(validation_loglik, as.factor(num_clusters))
  names(df) = c('Log-likelihood','Number of clusters')
  df_mean = df %>% 
    group_by(`Number of clusters`) %>% 
    summarise(Mean = mean(`Log-likelihood`))
  
  # plot
  p = df %>% 
    ggplot(aes(x=`Number of clusters`, y=`Log-likelihood`)) +
    geom_boxplot() +
    geom_point(data = df_mean, aes(`Number of clusters`, y=`Mean`, colour = 'Mean'), show.legend = TRUE, size = 3, shape = 18) +
    scale_color_manual(values = c("Mean" = 'cyan4')) +
    labs(color = "") +
    theme_bw()
  return(p)
}

```

# Data loading

To start, we will work only with 2 variables: Fixed acidity and Alcohol.  We can see in the following graph that the 3 clusters overlap each other in some regions. 
We plot initial clusters defined by the ellipses with 95% Confidence Interval, whose parameters are defined by the mean (centers) and the covariance matrix in each cluster. 

```{r cluster data, fig.height=4}

data(wine)
K = 3
X = as.matrix(wine[,c(2,4)])
y = wine[,1]
y_text = c("1 Barolo", "2 Grignolino", "3 Barbera")[y]
plt1 = plot_ellipses_CI(X,K,y_text) +
  xlab(colnames(X)[1]) +
  ylab(colnames(X)[2]) 
plt1 +  ggtitle('Fixed Acidity and Alcohol of the wines grouped by the true clusters')

```

# The EM algorithm

## Clustering comparison 

Let us use the EM algorithm in our bivariate dataset using the two types of initialization: random centroids and k-means 
For each initialization run the EM algorithm with values of $K = 2,3,4,5$ clusters
To measure how similar is the clustering to the ground truth we use the Adjusted Rand Index (ARI) and the Classification error (CE).

The Adjusted Rand Index derives from the Rand Index. 
Both measure the similarity between two partitions of the data.
These partitions can separate the data in different number of groups.
For instance, partition A can separate the data in 2 groups, while partition B separates it in 5 groups. 
This is ideal given that we want to compare the 3 true groups agains different number of clusters.
The ARI is 0 in the case of random partition, and 1 in the case of perfect agreement between two partitions.
Whereas the CE computes the error rate of EM clustering relative to the original clustering.
Based on these metrics we can select the clustering with better performance as the one with values ARI near 1 and CE near 0.

```{r ellipses cluster 2var, fig.height=18, fig.width=12}

n = dim(X)[1]

plots_list=list()
c_error_vec = c()
adj_rndI_vec = c()
mu_list = list()
loglik_list = list()
AIC_list = c()
BIC_list = c()
plt_n = 1


for(K in c(2:5)){
  for(init_EM in c("random_centroids", "k_means")){
    
results_EM = EM_algorithm(X, K, initialization = init_EM)
mu_em = results_EM$mu
mu_vec = results_EM$mu_hist
sigma_em = results_EM$sigma

groups_pred = classify(X,K,mu_em,sigma_em, prop = results_EM$prop)

#evaluating the clustering
ARI = adjustedRandIndex(groups_pred, y) %>% round(4) %>% as.character()
CE = classError(groups_pred, y)$errorRate %>% round(4) %>% as.character()
adj_rndI_vec = append(adj_rndI_vec,ARI)
c_error_vec = append(c_error_vec, CE)

#building the list of plots
p1 <- plot_ellipses(X,K,mu_em,sigma_em,groups_pred)

if(K==3 & init_EM=="random_centroids"){
  plt2 <- p1
}
if(K==3 & init_EM=="k_means"){
  plt3 <- p1
}

if (K==2 & init_EM=="random_centroids"){
  p1<-p1 + labs(title = "Initialization: Random centroids", subtitle = paste("K = 2"," ARI = ", ARI, " CE = ", CE))
} else if (K==2 & init_EM=="k_means"){
  p1<-p1 + labs(title = "Initialization: K-means", subtitle = paste("K = 2"," ARI = ", ARI, " CE = ", CE))
} else{
  p1<-p1 + labs(subtitle = paste("K = ", as.character(K)," ARI = ", ARI, " CE = ", CE))
}
plots_list[[plt_n]] = p1


#acummulate of mu_hist and loglik_hist for all the runs
mu_list[[plt_n]] = mu_vec
loglik_list[[plt_n]] = results_EM$loglik_hist
AIC_list = append(AIC_list, results_EM$aic)
BIC_list = append(BIC_list, results_EM$bic)

plt_n <- plt_n + 1

}
  
}

plt_final <- ggarrange(plotlist = plots_list, nrow = 4, ncol = 2, labels = '' ,legend = "none",   hjust = -0.6, vjust = 3.5, font.label = list(size = 12, color = "orangered", face = "plain", family = NULL)) 

annotate_figure(plt_final, top = text_grob("Final clusters determined by GMM using the EM algorithm by number of clusters and type of initialization \n Metrics to evaluate the clustering ARI = Adjusted Rand Index, CE = Class error", color = "black", face = "bold", size = 12))

```

In general we can say that the the initialization with random centroids has better performance in clustering the bivariate wine dataset. 
It is clear that the option with three clusters and random centroids initialization is the one that emulates the most the truth groups. 
This one shows also good values of the ARI and CE compared to the others. 

For larger values of $K$ the clustering with random centroids starts doing erratic clusterings, while the k-means keeps a similar pattern. 

| Initialization | K  | ARI  | CE |
| --- | --- | --- | --- |
| Random centroids | 2 | `r adj_rndI_vec[1]` | `r c_error_vec[1]` |
| K-means | 2 | `r adj_rndI_vec[2]` | `r c_error_vec[1]` |
| Random centroids | 3 | ** `r adj_rndI_vec[3]` ** | ** `r c_error_vec[1]`  ** |
| K-means | 3| `r adj_rndI_vec[4]` | `r c_error_vec[1]` |
| Random centroids | 4 | `r adj_rndI_vec[5]` | `r c_error_vec[1]` |
| K-means | 4 | `r adj_rndI_vec[6]` | `r c_error_vec[1]` |
| Random centroids | 5 | `r adj_rndI_vec[7]` | `r c_error_vec[1]` |
| K-means | 5 | `r adj_rndI_vec[8]` | `r c_error_vec[1]` |


## Mean convergence

Let us analyze how $\mu$ evolves from their initialization to their final positions. 


```{r mu plots, fig.height=12, fig.width=12, message=FALSE, warning=FALSE}

K=2
plots_list = c()
for (plt_n in c(1:length(mu_list))){
data_mu = mu_list[[plt_n]]

# plots_list = plot_hist_mu(data_mu, K)

p1 <- plot_hist_mu(data_mu, K)
p1_1<- p1[[1]] + ylab(colnames(X)[1]) + xlab('Iteration')
p1_2<- p1[[2]] + ylab(colnames(X)[2]) + xlab('Iteration')
if (plt_n%%2 == 1 & K==2){
  p1_1<-p1_1 + labs(title = "Init: Random centroids") 
  p1_2<-p1_2 + labs(subtitle = paste("K =", as.character(K),sep="")) 
  
}else if(plt_n%%2 == 0 & K==2){
  p1_1<-p1_1 + labs(title = "Init: K-means") 
  p1_2<-p1_2 + labs(subtitle = paste("K =", as.character(K),sep="")) 
} 

else{
  p1_1<-p1_1 + labs(subtitle = paste("K =", as.character(K),sep="")) 
  p1_2<-p1_2 + labs(subtitle = paste("K =", as.character(K),sep="")) 

}
p1[[1]] <- p1_1
p1[[2]] <- p1_2


plots_list = append(plots_list,p1)

if((plt_n %% 2) == 0) {
K<-K+1
}

}

plt_hist_mu <- ggarrange(plotlist = plots_list, nrow = 4, ncol = 4,legend = "none") 

annotate_figure(plt_hist_mu, top = text_grob("Historical values for the mu updates using the EM algorithm by number of clusters and type of initialization", color = "black", face = "bold", size = 12))
```
The convergence with K-means initialization is smother and faster than the random centroids, that is, its number of iterations in general is smaller. 
Which makes sense given that we have already run the k-means clustering to chooses the initial $\mu_i$.  

The final value of the means is different depending for the initialization.

Let us observe the final means K=3 in the table below.

```{r means for K=3 }
mu_rm = mu_list[[3]][dim(mu_list[[3]])[1],] %>% round(2) #means for k=3 random centroids init
mu_km = mu_list[[4]][dim(mu_list[[4]])[1],] %>% round(2) #means for k=3 k-means init
```


| Initialization | $K$  | $\mu_{1,\text{Alcohol}}$  | $\mu_{1,\text{Fixed Acidity}}$  | $\mu_{2,\text{Alcohol}}$   | $\mu_{2,\text{Fixed Acidity}}$  | $\mu_{3,\text{Alcohol}}$  | $\mu_{3,\text{Fixed Acidity}}$  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Random centroids | 3 | `r mu_rm[1]`  | `r mu_rm[4]` | `r mu_rm[2]` | `r mu_rm[5]` | `r mu_rm[3]` | `r mu_rm[6]` |
| K-means | 3 | `r mu_km[1]`  | `r mu_km[4]` | `r mu_km[2]` | `r mu_km[5]` | `r mu_km[3]` | `r mu_km[6]` |


# Log-likelihood convergence 

Now we observe the evolution of the log-likelihood for the two types of initialization. 

```{r loglik plots, fig.height=8, fig.width=9}

loglik_df <- plyr::ldply(loglik_list, rbind)
loglik_df <- data.frame(t(loglik_df[-1]))
plots_list = list()
for (cnt in c(1:4)){
  llk1<- loglik_df[,(2*cnt-1):(2*cnt)]
  llk1$index=c(1:nrow(llk1))
  
  df10 <- melt(llk1 ,  id.vars = 'index', variable.name = 'mu_clusters')
  df10 = na.omit(df10)
  
  p1 <- ggplot(df10, aes(index, value)) + geom_point(aes(colour = variable), shape=4) + geom_path(aes(colour = variable))+ theme_bw() + ylab("loglik") + xlab("iteration") + scale_color_manual(labels = c("Random centroids", "k-means"), values = c("#F8766D", "#00BFC4")) + labs(subtitle = paste("K = ",as.character(cnt+1)))
  
  plots_list[[cnt]] = p1
  
}

plt_hist_loglik <- ggarrange(plotlist = plots_list, nrow = 2, ncol = 2,common.legend = TRUE) 

annotate_figure(plt_hist_loglik, top = text_grob("Historical values for the Log-likelihood using the EM algorithm by number of clusters and type of initialization", color = "black", face = "bold", size = 12))

```


The random centroid provides a higher log-likelihood in all the cases which confirms the observations we had in the historical $\mu$ and the values of ARI and CE. With this analysis, we can say that the algorithm of EM-GMM with random centroids outperforms the k-means initialization, although it tends to take more iterations to converge.

Moreover we can also compare the values of AIC and BIC for the 2 initializations with $K=3$. And as expected all the values for the random centroids are higher than the k-means, as we can see in the table below.

| Algorithm | Initialization | K  | AIC  | BIC | 
| --- | --- | --- | --- | --- |
| EM | Random centroids | 3 | `r AIC_list[3]` | `r BIC_list[3]` | 
| EM | K-means | 3 | `r AIC_list[4]` | `r BIC_list[3]` |


# EM algorithm vs k-means

```{r,fig.height=8, fig.width=10}

#evaluating the clustering
error_rate_vec1 <- c(0,c_error_vec[3:4],round(classError(kmeans_cl$cluster, y)[[2]],4))
adj_rndI_vec1 <- c(1,adj_rndI_vec[3:4],round(adjustedRandIndex(kmeans_cl$cluster, y),4))

#applying k-means
kmeans_cl = kmeans(X, centers=3, nstart = 100, iter.max = 25)
plt4 <- plot_ellipses_CI(X,K,kmeans_cl$cluster) + xlab(colnames(X)[1]) + ylab(colnames(X)[2]) + theme(legend.position = "none")

# plots 
plots_list=list()
plots_list[[1]] <- plt1 + labs(subtitle = paste("Initial clustering", "ARI = ", adj_rndI_vec1[1]," CE = ",error_rate_vec1[1]))
plots_list[[2]] <- plt2 + labs(subtitle = paste("EM random centroids init", "ARI = ", adj_rndI_vec1[2]," CE = ",error_rate_vec1[2]))
plots_list[[3]] <- plt3 + labs(subtitle = paste("EM K-means init","ARI = ", adj_rndI_vec1[3]," CE = ",error_rate_vec1[3]))
plots_list[[4]] <- plt4 + labs(subtitle = paste("K-means clustering", "ARI = ", adj_rndI_vec1[4]," CE = ",error_rate_vec1[4]))

# labels_error <- paste("ARI=", as.character(round(adj_rndI_vec1,4)), ", CE=", as.character(round(error_rate_vec1,4)),sep="")

plt_final <- ggarrange(plotlist = plots_list, nrow = 2, ncol = 2, common.legend = TRUE, labels = '', hjust = -0.6, vjust = 3.5, font.label = list(size = 12, color = "orangered", face = "plain", family = NULL)) 


annotate_figure(plt_final, top = text_grob("Comparison between EM algorithm with 2 initialization and the K-means clustering \n Metrics to evaluate the clustering ARI = Adjusted Rand Index, CE = Class error", color = "black", face = "bold", size = 12))


```

Considering the ARI and CE we can confirm that the EM algorithm with random centroids have the best performance. We observe also that the clustering given by the k-means is pretty similar to the EM algorith with k-means initialization, the mean values just change a bit with the iterations in the EM. Something to note is that the grouping made by k-means takes horizontal groups in our data, forming the groups along the x-axis (alcohol), which is the variable that has the lowest variance.

# Number of clusters selection

To select the number of clusters we take into consideration 2 approaches: the information criteria (AIC and BIC), and the likelihood in the validation set using cross-validation.

We experiment extracting from 2 to 7 clusters. 

For the first approach, we apply the EM-algorithm setting different number of clusters and observe the resulting AIC and BIC.

For the second approach, we implement k-fold cross-validation and measure the log-likelihood in each validation sample.    


```{r number of cluster selection for bivariate case, fig.height=4, fig.width=12}

# Empty vectors to safe results from cross-validation 
validation_loglik = c()
num_clusters=c()
k_fold = 5

# Maximum and minimum number of clusters to iterate over
min_clusters = 2
max_clusters = 7 

# Empty vector to safe the results from AIC and BIC
aic_vector = c()
bic_vector = c()

# EM-algorithm for different number of clusters
for (number_of_clusters in min_clusters:max_clusters) {

  # run EM-algorithm
  results = EM_algorithm(X, K = number_of_clusters, initialization = "random_centroids")
  # save AIC and BIC
  aic_vector = append(aic_vector,results$aic)
  bic_vector = append(bic_vector,results$bic)
   
  # run cross-validation
  logliks = crossvalidation.EM(X, K = number_of_clusters, k_fold = k_fold, initialization = "random_centroids", seed = 32)
  # save likelihood results from cross validation  
  validation_loglik = append(validation_loglik,logliks)
  num_clusters = append(num_clusters,rep(number_of_clusters,k_fold))
} 


# Plotting the results 

plots_list = list(plot_AIC_BIC_by_cluster(aic_vector, bic_vector, min_clusters, max_clusters),
                  plot_loglik_by_cluster(validation_loglik, num_clusters))
ggarrange(plotlist = plots_list, nrow = 1, ncol = 2) 

```

The log-likelihoods derived from cross-validation indicate that on average using 3 clusters produces the higher log-likelihood.
The AIC indicates that 7 clusters would be the best option, while the BIC suggests using 3 clusters. Let us remember that the AIC does not take into consideration the sample size ($n=178$ in our case), so it could be giving advantage to models with too many clusters without considering that the sample size is not that big. 

Let us extract 7 clusters and observe the results.

```{r EM for 7 clusters}
# EM to extract 7 clusters
EM_7clusters = EM_algorithm(X, K=7, initialization = "random_centroids")
# Get the final clusters
clusters = classify(X, K=7, mu = EM_7clusters$mu, sigma = EM_7clusters$sigma, prop = EM_7clusters$prop)
# Plot clusters
plot_ellipses(X,K=7,EM_7clusters$mu,EM_7clusters$sigma,clusters)
# Number of elements by cluster
table(clusters)
```
We see that one of the cluster has as little as only `r min(table(clusters))` observations, where probably, the model is over-fitting. 

Therefore, in this scenario we choose extracting 3 clusters as the best solution, which coincides with the ground truth that we knew. That is, that there are indeed 3 types of wine in the dataset. 

# Towards higher dimensions

In this section we apply the EM-algorithm using more variables of the dataset and verify if the number of clusters stays the same.

We repeat the same process as in the previous section. That is, using the information criteria and the cross-validation log-likelihood.

```{r number of cluster selection for higher dimensions case, fig.height=16, fig.width=12}

#Empty lists to save the results
validation_loglik_list = list()
aic_vector_list = list()
bic_vector_list = list()
counter = 1

# Maximum and minimum number of clusters to iterate over
min_clusters = 2
max_clusters = 7 

# Run the process each time adding an extra variable
# Starting from the first 3 variables and onward 
for (i in 4:8){
 
  # Define the data
  X = as.matrix(wine[,2:i])
  
  # Empty vectors to safe results from cross-validation 
  validation_loglik = c()
  num_clusters=c()
  k_fold = 5
  
  # Empty vector to safe the results from AIC and BIC
  aic_vector = c()
  bic_vector = c()
  
  # EM-algorithm for different number of clusters
  for (number_of_clusters in min_clusters:max_clusters) {
    
    # run EM-algorithm
    results = EM_algorithm(X, K = number_of_clusters, initialization = "random_centroids")
    # save AIC and BIC
    aic_vector = append(aic_vector,results$aic)
    bic_vector = append(bic_vector,results$bic)
     
    # run cross-validation
    logliks = crossvalidation.EM(X, K = number_of_clusters, k_fold = k_fold, initialization = "random_centroids", seed = 32)
    # save likelihood results from cross validation  
    validation_loglik = append(validation_loglik,logliks)
    num_clusters = append(num_clusters,rep(number_of_clusters,k_fold))
  }

  validation_loglik_list[[counter]] = validation_loglik
  aic_vector_list[[counter]] = aic_vector
  bic_vector_list[[counter]] = bic_vector
  
  counter = counter + 1
  
 
}


# Plot results

# Defining dimensions of the axis to have standard scale for all plots
max_axis = c(unlist(aic_vector_list),unlist(bic_vector_list)) %>% max()
min_axis = c(unlist(aic_vector_list),unlist(bic_vector_list)) %>% min()
edge_size = (max_axis-min_axis)/20
max_axis = max_axis + edge_size
min_axis = min_axis - edge_size

max_axis_ll = unlist(validation_loglik_list) %>% max()
min_axis_ll = unlist(validation_loglik_list) %>% min()
edge_size_ll = (max_axis_ll - min_axis_ll)/20
max_axis_ll = max_axis_ll + edge_size_ll
min_axis_ll = min_axis_ll - edge_size_ll

# Plots
plots_list = list()
for (i in 1:length(validation_loglik_list)){
  plots_list[[2*i-1]] = plot_AIC_BIC_by_cluster(aic_vector_list[[i]], bic_vector_list[[i]], min_clusters, max_clusters) + 
    ggtitle(paste(as.character(i+2),' features')) +
    ylim(min_axis, max_axis)
  plots_list[[2*i]] = plot_loglik_by_cluster(validation_loglik_list[[i]], num_clusters) +
    ggtitle(paste(as.character(i+2),' features')) +
    ylim(min_axis_ll, max_axis_ll)
}

ggarrange(plotlist = plots_list, nrow = length(validation_loglik_list), ncol = 2) %>% 
  annotate_figure(top = text_grob("AIC, BIC and cross-validation log-likelihood by number of clusters and features", color = "black", face = "bold", size = 12))

```


In most of the cases, the BIC and the cross-validation log-likelihood propose 2 clusters. 
On the other hand, the AIC is prompt to choose a higher number of clusters. 
This behavior coincides with what we observed in the bivariate case.

The separation between AIC and BIC increases when we add more variables to the model.
This occurs because the BIC applies a stronger penalty to less parsimonious models. 
Also, all the metrics, AIC, BIC and log-likelihood decrease when the number of variables increases, implying that adding these variables is not helping to improve the model. 
Nevertheless, using a stepwise variable selection process we could add some well-chosen variables that could improve the model. 

Let us observe the data for the first 7 variables and visualize the true clusters and the ones generated by the model when $K=3$.

```{r EM for 3 clusters 7 variables, fig.height=7, fig.width=12}

X = as.matrix(wine[,2:8])
results = EM_algorithm(X = X, K = 3, initialization = "random_centroids")
clusters = classify(X = X, K = 3, mu = results$mu, sigma = results$sigma, prop = results$prop)

my_cols <- c("#00AFBB", "#E7B800", "#FC4E07")
pairs(X,  pch = c(1,4,16)[clusters], cex = 0.8, col = my_cols[y], lower.panel=NULL, oma=c(3,3,10,3))
legend("top", col = my_cols, legend = levels(as.factor(c("Barolo", "Grignolino", "Barbera")[y])), pch = 20, xpd = NA, ncol = 3, bty = "n", inset = 0.01, pt.cex = 1.5)
legend("top", pch = 1:3, legend = levels(as.factor(clusters)), col = "black",  xpd = NA, ncol = 3, bty = "n", inset = -0.03)
title("First 7 variables of the wine dataset by the true clusters and the clusters generated by a GMM")
```

In the scatter matrix, except for the "alcohol" variable, we see that 2 of the true clusters (Barbera and Barolo) usually overlap.
It is probably for this reason that the BIC and the log-likelihood from cross-validation were suggesting extracting only 2 clusters.
