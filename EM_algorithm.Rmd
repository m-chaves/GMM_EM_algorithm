---
title: "EM algorithm for multivariate data"
author: "Mariana Chaves and Franz Franco Gallo"
output: html_document
---

# Introduction

In this work, we implement the EM-algorithm for GMM. It is applied to multivariate data, more specifically the wine dataset available in the pgmm package. This dataset contains chemical characteristics about three types of wine. We use AIC, BIC and the likelihood as metrics to identify an adequated number of clusters. To assess the quality of the clustering, we use the function classError and adjustedRandIndex from the Mclust package. We compare the results of GMM with another clustering algorithm (k-means), and against the ground truth, that is the original wine type.   


# Data loading

To start, we will work only with 2 variables: Fixed acidity and Alcohol.  
```{r}
library(pgmm)
library(mvtnorm)

data(wine)
X = as.matrix(wine[,c(2,4)])
y = wine[,1]
plot(X,col=y)
```

# Useful functions

In this section we define the functions that will help us in our analysis. 

* `logsumexp` computes the log of the sum of the exponential of each entry of a vector $x$. This is useful for the computation of the log-likelihood and the responsibilities $\gamma(z_{nk})$.
* We create two functions, `initialization.random_centroids` and `initialization.k_means`, for the initialization of the parameters $\mu_k$, $\sigma_k$ and $\pi_k$. Given $K$, the number of clusters, `initialization.random_centroids` randomly samples $K$ points of the data and sets them as the initialization of $\mu_1, \ldots , \mu_k$. The variance-covariance matrix of the data $X$ serves as initialization for each $sigma_k$. Each $\pi_k$ is set to $1/K$. The second function, `initialization.k_means`, runs k-means on the data $X$. Then it uses the final centroids produced by k-means to initialize each $\mu_k$. It computes the variance-covariance matrices of the clusters found by k-means to initialize each $\sigma_k$. Finally, each $pi_k$ is initiliazed according to the number of elements in the clusters.
* Given a dataset $X$ and a number of desired clusters $K$, `EM_algorithm` computes the EM algorithm. One of the previously described initializations must be chosen.       
* `classify` takes the resulting means ($\mu_1, \ldots, \mu_K$) and variances ($\sigma_1, \ldots, \sigma_K$) of the EM algorithm and classifies the points in $K$ clusters.
* `metrics` computes the likelihood, AIC and BIC. 
* `crossvalidation.EM` performs cross-validation on the EM-algorithm.  


```{r}
logsumexp = function(x) {
  "
  This function computes the log of the sum of the exponentials of each entry of a vector x
  
  Input:
  x: a vector x
  "
  y = max(x)
  y + log(sum(exp(x-y)))
}

#Initialization functions

initialization.random_centroids = function(X,K){
  "
  This function defines the initialization of mu, sigma and prop.
  
  To initialize the mus, K random data points for X are sampled.
  To initialize the sigmas, we use the variance-covariance matrix of X.
  The props are initialized as 1/K 
  
  Inputs:
  X: the data
  K: the number of groups
  "
  mu = X[c(sample.int(n=dim(X)[1], size=K)),]
  sigma = lapply(1:K, FUN = function(i) {var(X)})
  prop = rep(1/K,K) 
  
  return(list(mu = mu, sigma = sigma, prop = prop))
  
}


initialization.k_means = function(X,K){
  "
  This function defines the initialization of mu, sigma and prop.
  
  First k-means is applied to the data X to define K clusters. 
  The final centroids are used as initialization for mu.
  The variance-covariance matrix of each cluster are used as initialization for sigma. 
  The proportion of elements in each cluster are used as initialization for prop.
  
  Inputs:
  X: the data
  K: the number of groups
  "
  n = dim(X)[1]
  kmeans_result = kmeans(X, K)
  mu = as.matrix(kmeans_result$centers)
  sigma = lapply(1:K, FUN = function(i){var(X[kmeans_result$cluster == i,])})
  prop = kmeans_result$size / n
  
  return(list(mu = mu, sigma = sigma, prop = prop))  
}


metrics = function(X,K,mu,sigma,prop){
  "
  This function computes the log-likelihood, AIC and BIC for a GMM given the data, the props, the means, and variances of the gaussians.
  
  Inputs:
  X: the data
  K: the number of groups
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  
  Outputs:
  loglik: the final log-likelihood
  aic: AIC of the model
  bic: BIC of the model  
  "
  n = dim(X)[1] #number of observations
  d = dim(X)[2] #number of features
  eta = (K-1) + (K*d) + (K*d*(d-1)/2) #penalty term 
  
  # Compute Likelihood
  log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
  loglik = sum(apply(log_gamma_numerator, 1, logsumexp))
  
  # Compute AIC and BIC
  aic = loglik - eta
  bic = loglik - 0.5 * eta * log(n)
  
  return(list(loglik = loglik,
              aic = aic,
              bic = bic))
} 

EM_algorithm = function(X,K,max_it=1000, initialization, number_of_initialization = 10){
  
  "
  This function computes the EM-algorithm
  
  Note that the EM-algorith is sensible to the initialization of the parameters. 
  That is, different initializations can let to different local maxima of the log-likelihood.
  'number of initialization' defines the number of times to run the EM-algorithm with different initializations.
  The function outputs the results related to the best maxima found. 
  
  Inputs:
  X: the data
  K: the number of groups
  max_it: maximum of iterations (defaults to 1000)
  initialization: 'random centroids' or 'k_means'
  number_of_initialization: 
  
  Outputs:
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  loglik: the final log-likelihood
  loglik_hist: a vector with the log-likelihood in each iteration
  aic: AIC of the model
  bic: BIC of the model
  "
  
  X = as.matrix(X)
  n = dim(X)[1] #number of observations
  
  best_maximum = -Inf
  for (j in 1:number_of_initialization){
    
    gamma = matrix(NA,n,K)
    prev_loglik = -Inf
    loglik_hist = c() # vector to save history of log-likelihood
      
    # Initialization of parameters
    if (initialization == "random_centroids"){
      init.param = initialization.random_centroids(X,K)
    } else if (initialization == "k_means") {
      init.param = initialization.k_means(X,K)
    }
    mu = as.matrix(init.param$mu)
    sigma = init.param$sigma
    prop = init.param$prop
    
    
    for (i in 1:max_it){
      
      # S step
      log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
      log_gamma = log_gamma_numerator - apply(log_gamma_numerator, 1, logsumexp)
      gamma = exp(log_gamma)
    
      # M step
       for (k in 1:K){
        nk = sum(gamma[,k])
        prop[k] = nk/n
        mu[k,] = colSums(gamma[,k]*X)/nk
        sigma[[k]] = t(sweep(X,2,mu[k,]))%*%diag(gamma[,k])%*%(sweep(X,2,mu[k,])) / nk 
       }
      
      # Compute Likelihood, AIC and BIC
      results_metrics = metrics(X,K,mu,sigma,prop)
      loglik = results_metrics$loglik
      aic = results_metrics$aic
      bic = results_metrics$bic
      
      loglik_hist = append(loglik_hist,loglik)
      
    
      # Stop process if likelihood didn't change
      if (prev_loglik == loglik){break}
      
      prev_loglik = loglik
    
    }
    
    results = list(mu = mu,
                sigma = sigma,
                prop = prop,
                loglik = loglik,
                loglik_hist = loglik_hist,
                aic = aic,
                bic = bic
                )
  
  # If this initialization found a better maxima, save the results and new best maximum. 
  if (best_maximum < results$loglik)
    {
    best_maximum = results$loglik
    best_results = results
    }
  
  }
  
  return(best_results)
  
  
}

classify = function(X,K,mu,sigma,prop){
   "
  This funtion classifies the data points according to the results of the EM-algorithm
  
  Inputs:
  X: the data
  K: the number of groups
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  
  Outputs:
  a vector indicating the cluster to which the point belongs
  "
  
  log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
  log_gamma = log_gamma_numerator - apply(log_gamma_numerator, 1, logsumexp)
  gamma = exp(log_gamma)
  cluster = sapply(1:n, FUN = function(i){which.max(gamma[i,])})
  
  return(cluster)
}

crossvalidation.EM = function(X, K, k_fold, max_it=1000, initialization, seed = 32){
  "
  This function perform k-fold cross validation on the EM-algorithm. 
  
  Inputs:
  X: the data
  K: the number of clusters in the EM algorithm
  k_fold: the number of groups for cross validation
  max_it: maximum of iterations in the EM algorithm (defaults to 1000)
  initialization: 'random centroids' or 'k_means'
  seed: seed for the random process that assigns the datapoints to each fold 
  
  Outputs:
  logliks: vector with the loglikelihoods obtained in the validation sets
  "
  
  # Assigning each datapoint to one of k groups
  X = as.matrix(X)
  n = dim(X)[1]
  vector_groups = ceiling((1:n)/(n/k_fold))
  set.seed(seed)
  vector_groups = sample(vector_groups,n)
  
  # Variables to save results
  logliks = c()
  
  for (i in 1:k_fold){
    # training and validation split
    train = X[vector_groups!=i,]
    validation = X[vector_groups==i,]
    
    #run EM algorithm on train set
    EM_results = EM_algorithm(X = train, K=K, max_it = max_it, initialization = initialization)
    
    #get likelihood on validation set
    metric_results = metrics(X = validation, K = K, mu = EM_results$mu, sigma = EM_results$sigma, prop = EM_results$prop)
    logliks = append(logliks,metric_results$loglik)
  }
  
  return(logliks)
  
}


```


# The EM algorithm

Let us use the EM algorithm in our bivariate dataset using the two types of initialization. 

```{r}
# pending!!!!!!!!!!!!

# visualize the results of the two types (plot with the clusters and the final means)

# are the means and sigmas the same?

# is the classification the same?

# is the AIC, BIC and loglik the same?

# compare the number of iterations to converge (use length(loglik_hist))

# (if the results are the same continue with only one of the two)

```

```{r}
# Example using the classify function 

# Delete this later!!!!!! (Replace for the nice plots Franz is working on)

results = EM_algorithm(X, K=3, initialization = "random_centroids")

results_clusters = classify(X, K, mu = results$mu, sigma = results$sigma, prop = results$prop)

plot(X, col=results_clusters)
points(results$mu,col="blue", pch=19)


```


# EM algorithm vs k-means

```{r}
# pending!!!!!!!!!!!

# comparison the clusters of each technique using classError and adjustedRandIndex from the Mclust package

# compare the results of both of them according to the ground truth


```

# Number of clusters selection

To select the number of clusters we take into consideration 2 approaches: the information criteria (AIC and BIC), and the likelihood in the validation set using cross-validation.

We experiment extracting from 2 to 7 clusters. 

For the first approach, we apply the EM-algorithm setting different number of clusters and observe the resulting AIC and BIC.

For the second approach, we implement k-fold cross-validation and measure the log-likelihood in each validation sample.    


```{r}

# Empty vectors to safe results from cross-validation 
validation_loglik = c()
num_clusters=c()
k_fold = 5

# Empty vector to safe the results from AIC and BIC
aic_vector = c()
bic_vector = c()

# EM-algorithm for different number of clusters
for (number_of_clusters in 2:7) {

  # run EM-algorithm
  results = EM_algorithm(X, K = number_of_clusters, initialization = "random_centroids")
  # save AIC and BIC
  aic_vector = append(aic_vector,results$aic)
  bic_vector = append(bic_vector,results$bic)
   
  # run cross-validation
  logliks = crossvalidation.EM(X, K = number_of_clusters, k_fold = k_fold, initialization = "random_centroids", seed = 32)
  # save likelihood results from cross validation  
  validation_loglik = append(validation_loglik,logliks)
  num_clusters = append(num_clusters,rep(number_of_clusters,k_fold))
} 

```

```{r}
boxplot(validation_loglik~num_clusters)
plot(aic_vector~c(2:7))
plot(bic_vector~c(2:7))
```

# Towards higher dimensions

In this section we apply the EM-algorithm using more variables of the dataset and verify if the number of clusters stays the same. 

```{r}
# pending!!!

# do a loop to apply cross-validation and select the number of clusters for 3, 4, 5 ,6 ....10 variables, and then for the whole dataset maybe? (28 variables)
```




