---
title: "EM algorithm for multivariate data"
author: "Mariana Chaves and Franz Franco Gallo"
output: html_document
---

# Introduction

In this work, we implement the EM-algorithm for GMM. It is applied to multivariate data, more specifically the wine dataset available in the pgmm package. This dataset contains chemical characteristics about three types of wine. We use AIC, BIC and the likelihood as metrics to identify an adequated number of clusters. To assess the quality of the clustering, we use the function classError and adjustedRandIndex from the Mclust package. We compare the results of GMM with another clustering algorithm (k-means), and against the ground truth, that is the original wine type.   

```{r libraries}
library(pgmm)
library(ellipse)
library(ggplot2)
library(ggpubr)
library(mclust)
library(mvtnorm)
```



# Useful functions

In this section we define the functions that will help us in our analysis. 

* `logsumexp` computes the log of the sum of the exponential of each entry of a vector $x$. This is useful for the computation of the log-likelihood and the responsibilities $\gamma(z_{nk})$.
* We create two functions, `initialization.random_centroids` and `initialization.k_means`, for the initialization of the parameters $\mu_k$, $\sigma_k$ and $\pi_k$. Given $K$, the number of clusters, `initialization.random_centroids` randomly samples $K$ points of the data and sets them as the initialization of $\mu_1, \ldots , \mu_k$. The variance-covariance matrix of the data $X$ serves as initialization for each $sigma_k$. Each $\pi_k$ is set to $1/K$. The second function, `initialization.k_means`, runs k-means on the data $X$. Then it uses the final centroids produced by k-means to initialize each $\mu_k$. It computes the variance-covariance matrices of the clusters found by k-means to initialize each $\sigma_k$. Finally, each $pi_k$ is initiliazed according to the number of elements in the clusters.
* Given a dataset $X$ and a number of desired clusters $K$, `EM_algorithm` computes the EM algorithm. One of the previously described initializations must be chosen.     
* `classify` takes the resulting means ($\mu_1, \ldots, \mu_K$) and variances ($\sigma_1, \ldots, \sigma_K$) of the EM algorithm and classifies the points in $K$ clusters.
* `metrics` computes the likelihood, AIC and BIC. 
* `crossvalidation.EM` performs cross-validation on the EM-algorithm.  


```{r functions}
logsumexp = function(x) {
  "
  This function computes the log of the sum of the exponentials of each entry of a vector x
  
  Input:
  x: a vector x
  "
  y = max(x)
  y + log(sum(exp(x-y)))
}

#Initialization functions

initialization.random_centroids = function(X,K){
  "
  This function defines the initialization of mu, sigma and prop.
  
  To initialize the mus, K random data points for X are sampled.
  To initialize the sigmas, we use the variance-covariance matrix of X.
  The props are initialized as 1/K 
  
  Inputs:
  X: the data
  K: the number of groups
  "
  mu = X[c(sample.int(n=dim(X)[1], size=K)),]
  sigma = lapply(1:K, FUN = function(i) {var(X)})
  prop = rep(1/K,K) 
  
  return(list(mu = mu, sigma = sigma, prop = prop))
  
}


initialization.k_means = function(X,K){
  "
  This function defines the initialization of mu, sigma and prop.
  
  First k-means is applied to the data X to define K clusters. 
  The final centroids are used as initialization for mu.
  The variance-covariance matrix of each cluster are used as initialization for sigma. 
  The proportion of elements in each cluster are used as initialization for prop.
  
  Inputs:
  X: the data
  K: the number of groups
  "
  n = dim(X)[1]
  kmeans_result = kmeans(X, K)
  mu = as.matrix(kmeans_result$centers)
  sigma = lapply(1:K, FUN = function(i){var(X[kmeans_result$cluster == i,])})
  prop = kmeans_result$size / n
  
  return(list(mu = mu, sigma = sigma, prop = prop))  
}


metrics = function(X,K,mu,sigma,prop){
  "
  This function computes the log-likelihood, AIC and BIC for a GMM given the data, the props, the means, and variances of the gaussians.
  
  Inputs:
  X: the data
  K: the number of groups
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  
  Outputs:
  loglik: the final log-likelihood
  aic: AIC of the model
  bic: BIC of the model  
  "
  n = dim(X)[1] #number of observations
  d = dim(X)[2] #number of features
  eta = (K-1) + (K*d) + (K*d*(d-1)/2) #penalty term 
  
  # Compute Likelihood
  log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
  loglik = sum(apply(log_gamma_numerator, 1, logsumexp))
  
  # Compute AIC and BIC
  aic = loglik - eta
  bic = loglik - 0.5 * eta * log(n)
  
  return(list(loglik = loglik,
              aic = aic,
              bic = bic))
} 

EM_algorithm = function(X,K,max_it=1000, initialization, number_of_initialization = 10){
  
  "
  This function computes the EM-algorithm
  
  Note that the EM-algorith is sensible to the initialization of the parameters. 
  That is, different initializations can let to different local maxima of the log-likelihood.
  'number of initialization' defines the number of times to run the EM-algorithm with different initializations.
  The function outputs the results related to the best maxima found. 
  
  Inputs:
  X: the data
  K: the number of groups
  max_it: maximum of iterations (defaults to 1000)
  initialization: 'random centroids' or 'k_means'
  number_of_initialization: 
  
  Outputs:
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  loglik: the final log-likelihood
  loglik_hist: a vector with the log-likelihood in each iteration
  aic: AIC of the model
  bic: BIC of the model
  "
  
  X = as.matrix(X)
  n = dim(X)[1] #number of observations
  
  best_maximum = -Inf
  for (j in 1:number_of_initialization){
    
    gamma = matrix(NA,n,K)
    prev_loglik = -Inf
    loglik_hist = c() # vector to save history of log-likelihood
      
    # Initialization of parameters
    if (initialization == "random_centroids"){
      init.param = initialization.random_centroids(X,K)
    } else if (initialization == "k_means") {
      init.param = initialization.k_means(X,K)
    }
    mu = as.matrix(init.param$mu)
    sigma = init.param$sigma
    prop = init.param$prop
    
    
    for (i in 1:max_it){
      
      # S step
      log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
      log_gamma = log_gamma_numerator - apply(log_gamma_numerator, 1, logsumexp)
      gamma = exp(log_gamma)
    
      # M step
       for (k in 1:K){
        nk = sum(gamma[,k])
        prop[k] = nk/n
        mu[k,] = colSums(gamma[,k]*X)/nk
        sigma[[k]] = t(sweep(X,2,mu[k,]))%*%diag(gamma[,k])%*%(sweep(X,2,mu[k,])) / nk 
       }
      
      # Compute Likelihood, AIC and BIC
      results_metrics = metrics(X,K,mu,sigma,prop)
      loglik = results_metrics$loglik
      aic = results_metrics$aic
      bic = results_metrics$bic
      
      loglik_hist = append(loglik_hist,loglik)
      
    
      # Stop process if likelihood didn't change
      if (prev_loglik == loglik){break}
      
      prev_loglik = loglik
    
    }
    
    results = list(mu = mu,
                sigma = sigma,
                prop = prop,
                loglik = loglik,
                loglik_hist = loglik_hist,
                aic = aic,
                bic = bic
                )
  
  # If this initialization found a better maxima, save the results and new best maximum. 
  if (best_maximum < results$loglik)
    {
    best_maximum = results$loglik
    best_results = results
    }
  
  }
  
  return(best_results)
  
  
}

classify = function(X,K,mu,sigma,prop){
   "
  This funtion classifies the data points according to the results of the EM-algorithm
  
  Inputs:
  X: the data
  K: the number of groups
  mu: the mean of each gaussian
  sigma: the variance-covariance matrix of each gaussian
  prop: the proportion for each group
  
  Outputs:
  a vector indicating the cluster to which the point belongs
  "
  
  log_gamma_numerator = sapply(1:K, FUN = function(k){log(prop[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})
  log_gamma = log_gamma_numerator - apply(log_gamma_numerator, 1, logsumexp)
  gamma = exp(log_gamma)
  cluster = sapply(1:n, FUN = function(i){which.max(gamma[i,])})
  
  return(cluster)
}

crossvalidation.EM = function(X, K, k_fold, max_it=1000, initialization, seed = 32){
  "
  This function perform k-fold cross validation on the EM-algorithm. 
  
  Inputs:
  X: the data
  K: the number of clusters in the EM algorithm
  k_fold: the number of groups for cross validation
  max_it: maximum of iterations in the EM algorithm (defaults to 1000)
  initialization: 'random centroids' or 'k_means'
  seed: seed for the random process that assigns the datapoints to each fold 
  
  Outputs:
  logliks: vector with the loglikelihoods obtained in the validation sets
  "
  
  # Assigning each datapoint to one of k groups
  X = as.matrix(X)
  n = dim(X)[1]
  vector_groups = ceiling((1:n)/(n/k_fold))
  set.seed(seed)
  vector_groups = sample(vector_groups,n)
  
  # Variables to save results
  logliks = c()
  
  for (i in 1:k_fold){
    # training and validation split
    train = X[vector_groups!=i,]
    validation = X[vector_groups==i,]
    
    #run EM algorithm on train set
    EM_results = EM_algorithm(X = train, K=K, max_it = max_it, initialization = initialization)
    
    #get likelihood on validation set
    metric_results = metrics(X = validation, K = K, mu = EM_results$mu, sigma = EM_results$sigma, prop = EM_results$prop)
    logliks = append(logliks,metric_results$loglik)
  }
  
  return(logliks)
  
}

# Plotting functions

plot_ellipses = function(X,K,mu,sigma,groups_pred){
   "
  This funtion plot the ellipses of the GMM clustering defined by mu and sigma
  
  Inputs:
  X: data
  K: the number of clusters in the EM algorithm
  mu: mean computed by the EM algorithm
  sigma: cov matrix computed by the EM algorithm
  groups_pred: vector predicted by the EM with the cluster group for each data point
  
  Outputs:
  p: plot of the data with the GMM clusters(ellipses)
  "
df <- data.frame(X, groups_pred)
colnames(df) <- c("x1", "x2", "group")
for(g in c(1:K)){
  df$group[df$group==g] <- LETTERS[g]
}

#calculating ellipses

clusters <- sort(unique(df$group))
df_ell <- data.frame()
for(g in c(1:K)){
M=sigma[[g]]
centre=mu[g,]

df_ell <- rbind(df_ell, cbind(as.data.frame(ellipse(M, centre=centre)),group=clusters[g]))

}
colnames(df_ell) <- c("x1", "x2", "group")
#drawing
mu_df = data.frame(mu)
p <- ggplot(data=df, aes(x=x1, y=x2,colour=group)) + geom_point(size=1.5, alpha=.6) + geom_path(data=df_ell, aes(x=x1, y=x2,colour=group), size=1, linetype=2) +geom_point(data=mu_df,aes(x=mu_df[,1],y=mu_df[,2]), shape=4, color='black',size=3)
  
return(p)
}

plot_ellipses_CI = function(X,K,groups){
   "
    This funtion plot the ellipses defined by the data itself with a 95% CI
  
  Inputs:
  X: data
  K: the number of clusters
  groups: vector with the cluster group for each data point
  
  Outputs:
  p: plot of the data with its cluster
  "
df <- data.frame(X, groups)
colnames(df) <- c("x", "y", "group")
for(g in c(1:K)){
  df$group[df$group==g] <- LETTERS[g]
  
}

#calculating ellipses
clusters <- sort(unique(df$group))
df_ell <- data.frame()
for(g in c(1:K)){
df_ell <- rbind(df_ell, cbind(as.data.frame(with(df[df$group==clusters[g],], ellipse(cor(x, y), 
                                         scale=c(sd(x),sd(y)), 
                                         centre=c(mean(x),mean(y))))),group=clusters[g]))

}

#drawing
p <- ggplot(data=df, aes(x=x, y=y,colour=group)) + geom_point(size=1.5, alpha=.6) + geom_path(data=df_ell, aes(x=x, y=y,colour=group), size=1, linetype=2)
  
return(p)
}


```

# Data loading

To start, we will work only with 2 variables: Fixed acidity and Alcohol.  
```{r data loading}
data(wine)
X = as.matrix(wine[,c(2,4)])
y = wine[,1]
clPairs(X, y)

```


# Add some description of this graph (the one with ellipses) 

```{r cluster data, fig.height=4}

#WIP: name axis according to the variables, add groups names (the wine names) 

data(wine)
K = 3
X = as.matrix(wine[,c(2,4)])
y = wine[,1]
plot_ellipses_CI(X,K,y)


```


# The EM algorithm

Let us use the EM algorithm in our bivariate dataset using the two types of initialization. 

WIP: Explain what is the adjustedRandIndex and classError, provide the interpretations.

```{r ellipses cluster 2var, fig.height=18, fig.width=12}

# WIP: axis names, titles for the graphs, make nicer tooltip of the error (if possible) and report also the result of adjustedRandIndex. 

n = dim(X)[1]

plots_list=list()
c_error_vec = c()
adj_rndI_vec = c()
plt_n = 1
for(K in c(2:5)){
  for(init_EM in c("random_centroids", "k_means")){
    
results_EM = EM_algorithm(X, K, initialization = init_EM)
mu_em = results_EM$mu
sigma_em = results_EM$sigma
groups_pred = classify(X,K,mu_em,sigma_em, prop = results_EM$prop)

#building the list of plots
p1 <- plot_ellipses(X,K,mu_em,sigma_em,groups_pred)
plots_list[[plt_n]]=p1
plt_n <- plt_n + 1

#evaluating the clustering
adj_rndI_vec = append(adj_rndI_vec, adjustedRandIndex(groups_pred, y))
c_error_vec = append(c_error_vec, classError(groups_pred, y))
}
}

labels_error <- paste("error=", as.character(round(adj_rndI_vec,4)), sep="")

ggarrange(plotlist = plots_list, nrow = 4, ncol = 2, labels = labels_error ,legend = "none")

```


```{r}

# are the means and sigmas the same?



# is the classification the same?


# is the AIC, BIC and loglik the same?


# compare the number of iterations to converge (use length(loglik_hist))


# (if the results are the same continue with only one of the two)

```

# EM algorithm vs k-means

```{r}
# pending!!!!!!!!!!!



# comparison the clusters of each technique using classError and adjustedRandIndex from the Mclust package

# compare the results of both of them according to the ground truth


```

# Number of clusters selection

To select the number of clusters we take into consideration 2 approaches: the information criteria (AIC and BIC), and the likelihood in the validation set using cross-validation.

We experiment extracting from 2 to 7 clusters. 

For the first approach, we apply the EM-algorithm setting different number of clusters and observe the resulting AIC and BIC.

For the second approach, we implement k-fold cross-validation and measure the log-likelihood in each validation sample.    


```{r}

# Empty vectors to safe results from cross-validation 
validation_loglik = c()
num_clusters=c()
k_fold = 5

# Empty vector to safe the results from AIC and BIC
aic_vector = c()
bic_vector = c()

# EM-algorithm for different number of clusters
for (number_of_clusters in 2:7) {

  # run EM-algorithm
  results = EM_algorithm(X, K = number_of_clusters, initialization = "random_centroids")
  # save AIC and BIC
  aic_vector = append(aic_vector,results$aic)
  bic_vector = append(bic_vector,results$bic)
   
  # run cross-validation
  logliks = crossvalidation.EM(X, K = number_of_clusters, k_fold = k_fold, initialization = "random_centroids", seed = 32)
  # save likelihood results from cross validation  
  validation_loglik = append(validation_loglik,logliks)
  num_clusters = append(num_clusters,rep(number_of_clusters,k_fold))
} 

```

```{r}

#WIP: I need to implement nice graphs 

boxplot(validation_loglik~num_clusters)
points(tapply(validation_loglik, num_clusters, mean), col="blue", pch=19)
legend("bottomleft", inset=.02, legend=c("Mean"), col=c("blue"), pch=19, cex=0.8)
plot(aic_vector~c(2:7))
plot(bic_vector~c(2:7))

```
The log-likelihoods derived from cross-validation indicate that on average using 3 clusters produces the higher log-likelihood.
The AIC indicates that 7 clusters would be the best option, while the BIC suggests using 3 clusters. Let us remember that the AIC does not take into consideration the sample size ($n=178$ in our case), so it could be giving advantage to models with too many clusters without considering that the sample size is not that big. 

Let us extract 7 clusters and observe the results.

```{r}
EM_7clusters = EM_algorithm(X, K=7, initialization = "random_centroids")
clusters = classify(X, K=7, mu = EM_7clusters$mu, sigma = EM_7clusters$sigma, prop = EM_7clusters$prop)
table(clusters)

```
We see that one of the cluster has as little as only 4 observations, where probably, the model is over-fitting. 

Therefore, in this scenario we choose extracting 3 clusters as the best solution, which coincides with the ground truth that we knew. That is, that there are indeed 3 types of wine in the dataset. 

# Towards higher dimensions

In this section we apply the EM-algorithm using more variables of the dataset and verify if the number of clusters stays the same.

We repeat the same process as in the previous section. That is, using the information criteria and the cross-validation log-likelihood.

```{r}

#Empty lists to save the results
validation_loglik_list = list()
aic_vector_list = list()
bic_vector_list = list()
counter = 1

# Run the process each time adding an extra variable
# Starting from the first 3 variables and onward 
for (i in 4:10){
  print(i)
 
  # Define the data
  X = as.matrix(wine[,2:i])
  
  # Empty vectors to safe results from cross-validation 
  validation_loglik = c()
  num_clusters=c()
  k_fold = 5
  
  # Empty vector to safe the results from AIC and BIC
  aic_vector = c()
  bic_vector = c()
  
  # EM-algorithm for different number of clusters
  for (number_of_clusters in 2:7) {
    print(number_of_clusters)
    # run EM-algorithm
    results = EM_algorithm(X, K = number_of_clusters, initialization = "random_centroids")
    # save AIC and BIC
    aic_vector = append(aic_vector,results$aic)
    bic_vector = append(bic_vector,results$bic)
     
    # run cross-validation
    logliks = crossvalidation.EM(X, K = number_of_clusters, k_fold = k_fold, initialization = "random_centroids", seed = 32)
    # save likelihood results from cross validation  
    validation_loglik = append(validation_loglik,logliks)
    num_clusters = append(num_clusters,rep(number_of_clusters,k_fold))
  }

  validation_loglik_list[[counter]] = validation_loglik
  aic_vector_list[[counter]] = aic_vector
  bic_vector_list[[counter]] = bic_vector
  
  counter = counter + 1
  
 
}




```




